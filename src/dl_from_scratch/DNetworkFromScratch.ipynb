{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol0gkj9nT5U2"
      },
      "source": [
        "L’objectif de ce TP est d'implémenter par vous même l’apprentissage de réseaux de neurones simples. Cette prise en main sera formatrice pour utiliser des frameworks plus évolués (comme PyTorch) où l’apprentissage est automatisé.\n",
        "\n",
        "Nous allons utiliser la base de données image MNIST, constituée d’images de caractères manuscrits (60000 images en apprentissage, 10000 en test). L'objectif est de reconnaitre le chiffre par un réseau de neurones. Les images sont en niveau de gris de taille 28x28. Le vecteur d'entrée est de 784 valeurs et un vecteur de 10 valeurs en sortie.\n",
        "\n",
        "Voici le code pour récupérer les images avec sklearn :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS5kZaVC_rLs",
        "outputId": "0a742684-a9f3-4536-fd07-f24c2728ff0d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZqxNfbFS1OB",
        "outputId": "6b460ddc-6356-4801-8fa4-b98fda7355bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (70000, 784), y shape: (70000,)\n",
            "x: type= <class 'numpy.ndarray'>  shape= (70000, 784)\n",
            "y: type= <class 'numpy.ndarray'>  shape= (70000,)\n"
          ]
        }
      ],
      "source": [
        "def load_mnist_data():\n",
        "  # Télécharger les données MNIST avec sklearn\n",
        "  mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "  # Extraire les données et les étiquettes\n",
        "  X, y = mnist['data'], mnist['target']\n",
        "\n",
        "  # Conversion en arrays numpy\n",
        "  X = X.to_numpy().astype(np.float32)  # Convertir les features en float32 numpy array\n",
        "  y = y.to_numpy().astype(int)         # Convertir les labels en entiers\n",
        "\n",
        "  # Normalisation des données (optionnel)\n",
        "  X /= 255.0\n",
        "\n",
        "  # Reshape les images en 28x28 pour correspondre au format des images\n",
        "  #X = X.reshape(-1, 28, 28)\n",
        "  X = X.reshape(-1, 28*28)\n",
        "\n",
        "  # Afficher les formes des arrays\n",
        "  print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
        "  return X, y\n",
        "\n",
        "X,Y = load_mnist_data()\n",
        "print(\"x: type=\",type(X), \" shape=\",np.shape(X))\n",
        "print(\"y: type=\",type(Y), \" shape=\",np.shape(Y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WehJyJtZt5E"
      },
      "source": [
        "Visualisation de quelques images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "PSr7kyGlZxvu",
        "outputId": "a6f6d7a9-12a9-491e-ab23-1b2280b89df5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG8AAABvCAYAAADixZ5gAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEUklEQVR4nO2cSyg9URzH5/7JgjyyoZTEgpBsUFKSJMXCY6OsyIqysrGzIOWxEAsrZSNLjw0Lr4VS8tgoe7Lzfodr9/ufM7m3ucZwvsf3s/qOM3M6+nTOuTNz5oTC4XDYIZD8++0GkK9DecBQHjCUBwzlAUN5wFAeMJQHDOUBE+/1xFAoFGQ7iAsvD77Y84ChPGAoDxjKA4bygKE8YCgPGMoDhvKAoTxgKA8YygOG8oChPGA8vxIymbi4OO04NTXV03W9vb2SExMTJefn50vu6enRrhkbG5Pc3t4u+enpSTtvZGRE8uDgoKf2xAp7HjCUBwzlAWPcnJedna0dJyQkSK6srJRcVVUlOS0tTbumtbXVVxtOT08lT05OamXNzc2Sb29vJR8dHWnnbW1t+WqDF9jzgKE8YEJev88LcvVYaWmp5PX1da3M689+v7y/v0vu7OyUfHd3F/Ga8/NzyZeXl1rZycmJr/Zw9ZjlUB4wRgyb6enpknd3d7Wy3NxcX3Wr9V1dXWllNTU1kl9eXiT/1FAdDQ6blkN5wFAeMEY8Ybm4uJDc39+vlTU2Nko+ODiQ7H7yoXJ4eCi5rq5O8v39vXZeUVGR5L6+Pu8NNgT2PGAoDxgjbhWikZKSIll9EDwzMyO5q6tLu6ajo0Py/Px8gK0LDt4qWA7lAWPEr81o3NzcfPr36+vriNd0d3dLXlhYkKw+fLYB9jxgKA8Y439tRiIpKUny8vKyVlZdXS25oaFB8traWvAN+yb4a9NyKA8YygMGds5TycvL04739/clqy9gNzY2tPP29vYkT09PSzZh223OeZZDecBYMWy6UVc1z87OSk5OTo54zcDAgOS5uTnJ6vK+n4TDpuVQHjBWDpsqxcXFkicmJrSy2traT69R3xUODQ1pZWdnZ9/Yushw2LQcygOG8oCxfs5TcX+E2dTUJFm9pVD/V/dXS+pSwiDhnGc5lAfMnxo2o/H8/Cw5Pv7/0p7X11ftvPr6esmbm5uBtYfDpuVQHjDGL/3zS0lJieS2tjatrKysTLI6VKocHx9rx9vb29/YOn+w5wFDecBYMWyqu/Q5jr6bX0tLi+TMzExP9b29vUl2v88zadU1ex4wlAcM5QEDNeepc5a6y6w6xzmO4+Tk5MRct7oMUH0Bu7S0FHNdPwV7HjCUB4xxw2ZGRoZ2XFhYKHlqakpyQUFBzHW7t8YaHR2VvLi4KNmk24FosOcBQ3nA/Nqwqe70py61UzdOdZyv7fq3s7MjeXx8XPLq6qp23uPjY8x1mwR7HjCUBwzlARPonFdRUSHZvZtfeXm55KysrJjrfnh4kOzeAXB4eFiye6c/m2DPA4bygAl02FQ/clRzNNxrRlZWViSry/DUWwD3xt9/BfY8YCgPGK6YNhSumLYcygOG8oChPGAoDxjKA4bygKE8YCgPGM8Ppk3YQJTosOcBQ3nAUB4wlAcM5QFDecBQHjCUBwzlAfMBvQQ0FwRW2NAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 2800x2800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACwklEQVR4nO2Wv0vrUBiG35OTNKDoUK04tKAO4uDQQUTQpYPWRYR2UnB1c3IRQREcO/kPOKmL6CjFdmo2xbqJ+NtBEC0OFlu1ad673OVeezHNKV6RPHCmfJz34fuSnCNIEt8Y7X8LfIYvqIovqIovqMq3F9TdFgohGh7u5oz4OR2sByEEdF2HEAK2bcNxHM97NbyDhmFgYmIC2WwWR0dHmJubg6YpxNAlAD5dgUCAyWSSl5eXrFQqLBaLfHp6YjgcphDiQ72r3EYJNjU1MZlM8vT0lLZts1gscnNzk+fn59zb22M0GvUk2JARBwIBrK6uYmNjA5FIBHd3dzBNE47jIJ1OIxqNoru729Peyh+JYRiYnJxEIpGAbdvY3t6GZVkYHx/H1tYWbm9vMT09jZ6eHhiGgUqlUl+AyoiFEBweHqZlWSwUCsxkMhwYGGBrays7OjqoaRpN02SpVGIul2NXV9fXvoPt7e08ODjgzc0Np6amGAqF2Nzc/EeNaZosl8u8vr5mf39/3YJKI04kEujr60MqlcLu7i7e3t5q1gkhPJ9Enj+SSCSCmZkZPD4+IpfL/VNO13WQxMPDA15fX+vO8dzBlpYWBINBnJyc4PDwsGZNW1sb5ufncX9/j1Qqhaurq68TFELAcRycnZ2hVCp9eN7b24vZ2VnEYjGsrKwgm816OvKU3kFN0xAMBiGlhOM40DQNUkqMjo5iaWkJhmFgcXER+/v7rm4uDRcUQiAej2N5eRn5fB4jIyMYGxtDZ2cnjo+Psb6+DsuyPMsB8P4fDIfDzGQytG2b5XKZhUKB7+/vvLi44MLCQs2j7e/lKteroJSSg4OD3NnZ4cvLC5+fn7m2tsahoSGapunqcuEG8Tvc1ThrIaWElBIAUK1WUa1W65ne57mqgiq4if45V36XjW44376DvqAqvqAqvqAqvqAqvwDA5DqAyJQmDwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB/0lEQVR4nO2YvarqQBRGvyQGxQQEfyJOREMaLRVBEGx8BBtBELHxeXwAsRF7C59C7CzUTovYqIUIxmB0vJWnuVzOJNFz5JLVZiffYiYzexLu8Xg88MHwvy3wHb6gV3xBr/iCXvl4wQBrIcdxLw9n6REfP4K+IACkUilomgaedx7nSlDTNMTjcbYAnke73Uaz2UQwGHSc5VhQEAQ0Gg2USiWm+kAggFwuB0op06J4iWC9Xkc4HGaqLxaLqNVqWCwWsCzr/YLRaBSyLDPXt1otmKaJ9XrtNAqAC8FqtYpIJMI0XZIkIZvN4nQ6Ybvd/oxgPp+HaZowDOOfNYIggBCCbreLSqWC2WyG8/nsSpC5kzzhOA6WZeF4PAIARFFELBaDqqpQFAWEEGQyGei6jkKhAEmSMBqNcL1ef0bwcrlAVVX0ej3s93tIkgRd1xEKhWBZFjabDVarFYbDIQ6HA9LpNJbLpSs5AOBYP5qevVgURXQ6Hei6/nXNMAxMp1PM53NQSkEpBSEEg8EAiqKgXC7Dtu2/nskS7XgEbdtGv9//tk6WZRBCMB6Pcb/fncZ88fZWZxiGqw36yVsFeZ4HIcTTUe2tgpRSJJPJzxUEgEQi4el+x4uEld1uh8lkgtvt5ukddLzNvJL/4sjPPMW/9RPs40fQF/SKL+gVX9ArvqBX/gAPzaKZJZun4AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABzUlEQVR4nO2Wsc4pQRTHz7nuJpts0FBtsY1ELRpewNYKPIDGC3gChXcgKr1G4QEUChEFIpQKiSCRjLDE5n+7e4vL981nFlvsL5nuzNlfztk5MwwA5GN+fVrgOwJBVQJBVQJBVXwv+Fs2kJk9/7jMHeH7CvpeULrFzxCNRimZTJLrujSdTulyufw8CSQhoh8twzBQrVax3+8xHo9hWdZ/MTK8rMWxWIxyuRyFw2G6Xq9SB+Iur6hgJBJBvV6HEAL9fh+maSIUCj1VwZcIlkolHA4H7HY7FAoFMPPduI8IGoaBXq8Hx3HQbrcRj8cfxr5d0DAM1Go1CCEwHA6RTqe/jH+rIDPDtm1MJhOsVivk8/m7/93HBE3TxHK5xO12Q7fbha7r3+6RwZMxo2kaVSoVsiyLTqcTNRoNchzHi9TejBnbtjGfzyGEQKvV+vJg0LsryMyUzWYpkUjQer2mZrNJ2+1WNe1flAVTqRQVi0ViZlosFjSbzbzw+odKi5kZ5XIZ5/MZm80GmUzm4VC+t97SYl3XSdM0Oh6PNBqNnr9zH6D03AJAg8GAOp0Oua7ruRwREUMya/Dkf4B0i1/RPhl8X8FAUJVAUJVAUJVAUJU/oiX4gjQGE7MAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACM0lEQVR4nO2XMUvrUBiG39NeUpSi3UKHgEOgQqGDdMgglPwAl66dpUsWl/4AR7u4dmu3FkdL6CA6CBYnV0uz1EFKihKokFRp3zvdTW9ze+I1Qx4428n3PRzyni8RJIkYk/ppgXUkgrIkgrIkgrLEXvBX2I1CiMibh5kRsT/BRFCW2AuGDsk60uk0Dg8PcXR0hFKphNfXV/R6PaiqiqurKziOs1lhhgTAlyuTyfD4+JjPz890HIftdpuXl5f0PI++77PRaHz6XKi+soLZbJb1ep2TyYTX19c0TZPb29us1WqczWZ8eXmhYRg/I6goCk9PT+m6Lm3bZrFYpBCCmUyGw+GQHx8fbDabVBTl/wsKIdhoNDifz2nbNguFAlOpFPf29mhZFj3PY7fbZT6f//L0v1Vwa2uLNzc3fHx8pGEYPDg4oGVZHAwG9H2fjuOwUqlQCCEluHGKScL3faiqivPzc+zs7CCXy2G5XAIARqMR7u/vQ42zv7HxPRgEAU5OTnB7ewvXddFut2GaJlqtFoIgQL/fRxAEUnIAorlm/qxcLsdOp0Pf91kul9fuD0Okk+Tt7Q3T6RQkMZlMIqkZqeDu7i40TYuyZLSCsoH4jEgFPc/Dw8MD3t/foapqNEWjDAkA7u/v8+7ujhcXF9R1PV4hAYDxeIyzszNomoZqtQpFUaTqCYZ8cf7lnySdTkPXdSwWCzw9PWG1Wn26L0zr0JPkOwIQhth/USeCsiSCsiSCsiSCsvwGVjRmOu9A+HcAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def visu(X_train):\n",
        "  plt.figure(figsize=(28, 28))\n",
        "  for i in range(5):\n",
        "    plt.subplot(10,20,i+1)\n",
        "    plt.imshow( 255*(X_train[i,:].reshape([28,28])), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visu(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQmG0MICP3oY"
      },
      "source": [
        "Quelques fonctions utiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cCDUnc06P8Iv"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    \"\"\"Dérivée de la fonction sigmoid.\"\"\"\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "\n",
        "def to_one_hot(y, k):\n",
        "    \"\"\"Convertit un entier en vecteur \"one-hot\".\n",
        "    to_one_hot(5, 10) -> (0, 0, 0, 0, 1, 0, 0, 0, 0)\n",
        "    \"\"\"\n",
        "    one_hot = np.zeros(k)\n",
        "    one_hot[y] = 1\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "828p1_zgVlda"
      },
      "source": [
        "La définition d'un réseau complétement connecté (FC fuly connected) est une succession de couches (layer). Un layer contient une matrice W des poids et bias : W.x_input + bias. La fonction d'activation est sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iBBcYSg0VyU5"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "      Une seule couche de neurones.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, input_size):\n",
        "      \"\"\"\n",
        "        `size` est le nombre de neurones dans la couche.\n",
        "        `input_size` est le nombre de neurones dans la couche précédente.\n",
        "      \"\"\"\n",
        "      self.size = size\n",
        "      self.input_size = input_size\n",
        "\n",
        "      # Les poids sont représentés par une matrice de n lignes et m colonnes.\n",
        "      # n = le nombre de neurones, m = le nombre de neurones dans la couche précédente.\n",
        "      self.weights = np.random.randn(size, input_size)\n",
        "\n",
        "      # Un biais par neurone: y = W.x + B (B=bias)\n",
        "      self.bias = np.random.randn(size)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "      \"\"\"\n",
        "        Résultat du calcul de chaque neurone.\n",
        "        `data` est un vecteur de longueur `self.input_size`\n",
        "        retourne un vecteur de taille `self.size`.\n",
        "      \"\"\"\n",
        "      aggregation = self.aggregation(data)\n",
        "      activation = self.activation(aggregation)\n",
        "      return activation\n",
        "\n",
        "\n",
        "    def aggregation(self, data):\n",
        "      \"\"\"\n",
        "        Calcule W.data + bias\n",
        "      \"\"\"\n",
        "      return np.dot(self.weights, data) + self.bias\n",
        "\n",
        "\n",
        "    def activation(self, x):\n",
        "        \"\"\"\n",
        "          Passe les valeurs aggrégées dans la fonction d'activation.\n",
        "        \"\"\"\n",
        "        return sigmoid(x)\n",
        "\n",
        "\n",
        "    def activation_prime(self, x):\n",
        "        return sigmoid_prime(x)\n",
        "\n",
        "\n",
        "    def update_weights(self, gradient, learning_rate):\n",
        "        \"\"\" Mise à jour des poids à partir du gradient (algo du gradient) \"\"\"\n",
        "        self.weights -= learning_rate * gradient\n",
        "\n",
        "\n",
        "    def update_biases(self, gradient, learning_rate):\n",
        "        \"\"\" Idem mais avec les biais \"\"\"\n",
        "        self.bias -= learning_rate * gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olq1NgYFjPEy"
      },
      "source": [
        "Le réseau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uFwgMlGPjONR"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "    \"\"\"Un réseau constitué de couches de neurones.\"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.layers = []\n",
        "\n",
        "\n",
        "    def add_layer(self, size):\n",
        "        if len(self.layers) > 0:\n",
        "            input_dim = self.layers[-1].size\n",
        "        else:\n",
        "            input_dim = self.input_dim\n",
        "        self.layers.append(Layer(size, input_dim))\n",
        "\n",
        "\n",
        "    def feedforward(self, input_data):\n",
        "        \"\"\" Propage les données d'entrée d'une couche à l'autre \"\"\"\n",
        "        activation = input_data\n",
        "        for layer in self.layers:\n",
        "            activation = layer.forward(activation)\n",
        "        return activation\n",
        "\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        \"\"\" Passe input_data dans le réseau (feedForward) et retourne l'index du neurone de \n",
        "            sortie qui a la plus grande valeur (qui est la classe sélectionnée par le réseau). \"\"\"\n",
        "        return np.argmax(self.feedforward(input_data))\n",
        "\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        \"\"\" Évalue la performance du réseau à partir d'un set d'exemples. Retourne un nombre entre 0 et 1.\"\"\"\n",
        "        results = 0.0\n",
        "        n = 0\n",
        "        for (x,y) in zip(X,Y):\n",
        "            results += (self.predict(x.reshape( 28*28 )  ) == y)\n",
        "            n += 1\n",
        "        #results = [1 if self.predict(x) == y else 0 for (x, y) in zip(X, Y)]\n",
        "        accuracy = results / n\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "    def train(self, X, Y, steps=30, learning_rate=0.3, batch_size=10):\n",
        "        \"\"\"\n",
        "            Fonction d'entraînement du modèle.\n",
        "            La rétropropagation tourne sur un certain nombre d'exemples (batch_size) avant \n",
        "            de calculer un gradient moyen, et de mettre à jour les poids.\n",
        "        \"\"\"\n",
        "        n = Y.size\n",
        "        for i in range(steps):\n",
        "            X, Y = shuffle(X, Y)\n",
        "            for batch_start in range(0, n, batch_size):\n",
        "                X_batch, Y_batch = X[batch_start:batch_start + batch_size], Y[batch_start:batch_start + batch_size]\n",
        "                self.train_batch(X_batch, Y_batch, learning_rate)\n",
        "\n",
        "\n",
        "    def train_batch(self, X, Y, learning_rate):\n",
        "        \"\"\"     Cette fonction combine les algos du retropropagation du gradient + gradient descendant \"\"\"\n",
        "        # Initialise les gradients pour les poids et les biais.\n",
        "        weight_gradient = [np.zeros(layer.weights.shape) for layer in self.layers]\n",
        "        bias_gradient = [np.zeros(layer.bias.shape) for layer in self.layers]\n",
        "\n",
        "        # On fait tourner l'algo de rétropropagation pour calculer les\n",
        "        # gradients un certain nombre de fois. On fera la moyenne ensuite.\n",
        "        for (x, y) in zip(X, Y):\n",
        "            new_weight_gradient, new_bias_gradient = self.backprop(x, y)\n",
        "            weight_gradient = [wg + nwg for wg, nwg in zip(weight_gradient, new_weight_gradient)]\n",
        "            bias_gradient = [bg + nbg for bg, nbg in zip(bias_gradient, new_bias_gradient)]\n",
        "\n",
        "        # C'est ici qu'on calcule les moyennes des gradients calculés\n",
        "        avg_weight_gradient = [wg / Y.size for wg in weight_gradient]\n",
        "        avg_bias_gradient = [bg / Y.size for bg in bias_gradient]\n",
        "\n",
        "        # Il ne reste plus qu'à mettre à jour les poids et biais en utilisant l'algo du gradient descendant.\n",
        "        for layer, weight_gradient, bias_gradient in zip(self.layers, avg_weight_gradient, avg_bias_gradient):\n",
        "            layer.update_weights(weight_gradient, learning_rate)\n",
        "            layer.update_biases(bias_gradient, learning_rate)\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"   L'algorithme de rétropropagation du gradient. C'est là que tout le boulot se fait. \"\"\"\n",
        "        # Une passe vers l'avant puis une passe vers l'arrière\n",
        "        # On profite de la passe vers l'avant pour stocker les calculs\n",
        "        # intermédiaires, qui seront réutilisés par la suite.\n",
        "        aggregations = []\n",
        "        activation = x\n",
        "        activations = [activation]\n",
        "\n",
        "        # Propagation pour obtenir la sortie (même code que feedForward mais on stocke les valeurs intermédiaires dans aggregations et activations)\n",
        "        for layer in self.layers:\n",
        "            aggregation = layer.aggregation(activation)\n",
        "            aggregations.append(aggregation)\n",
        "            activation = layer.activation(aggregation)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Calcul de delta pour la dernière couche (delta = erreur)\n",
        "        target = to_one_hot(int(y), 10)\n",
        "        delta = self.get_output_delta(aggregation, activation, target)\n",
        "        deltas = [delta]\n",
        "\n",
        "        # Phase de rétropropagation pour calculer les deltas de chaque couche\n",
        "        # On utilise une implémentation vectorielle des équations.\n",
        "        nb_layers = len(self.layers)\n",
        "        for l in reversed(range(nb_layers - 1)):\n",
        "            layer = self.layers[l]\n",
        "            next_layer = self.layers[l + 1]\n",
        "            activation_prime = layer.activation_prime(aggregations[l])\n",
        "            delta = activation_prime * np.dot(next_layer.weights.transpose(), delta)\n",
        "            deltas.append(delta)\n",
        "\n",
        "        # Départ de l'avant-dernière couche pour remonter vers la première. \n",
        "        # deltas[0] contient le delta de la dernière couche.\n",
        "        # Il faut inverser pour faciliter la gestion des indices plus tard.\n",
        "        deltas = list(reversed(deltas))\n",
        "\n",
        "        # On utilise maintenant les deltas pour calculer les gradients.\n",
        "        weight_gradient = []\n",
        "        bias_gradient = []\n",
        "        for l in range(len(self.layers)):\n",
        "            # L'indice des activations est « décalé », puisque\n",
        "            # activation[0] contient l'entrée (x), et pas l'activation de la première couche.\n",
        "            prev_activation = activations[l]\n",
        "            weight_gradient.append(np.outer(deltas[l], prev_activation))\n",
        "            bias_gradient.append(deltas[l])\n",
        "        return weight_gradient, bias_gradient\n",
        "\n",
        "    # Calcule le delta pour la dernière couche, en utilisant\n",
        "    # les dernières valeurs d'aggregation, d'activation, et la valeur cible.\n",
        "    # Notez que lorsque l'on utilise l'entropie croisée pour fonction de\n",
        "    # coût, l'équation de calcul de delta peut-être simplifiée pour aboutir\n",
        "    # au résultat ci dessous.\n",
        "    # Cf http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function\n",
        "    def get_output_delta(self, computed, target):\n",
        "        return computed - target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8k7-lS8k7AG"
      },
      "source": [
        "Le main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "89htwB4gk8YY",
        "outputId": "d8cc922b-7f82-4762-8c68-f2e53740ee31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance initiale : 8.55%\n",
            "Nouvelle performance : 93.30%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[46], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance initiale : \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100.0\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mevaluate(X_test, Y_test)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNouvelle performance : \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100.0\u001b[39m))\n",
            "Cell \u001b[1;32mIn[45], line 53\u001b[0m, in \u001b[0;36mNetwork.train\u001b[1;34m(self, X, Y, steps, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n, batch_size):\n\u001b[0;32m     52\u001b[0m     X_batch, Y_batch \u001b[38;5;241m=\u001b[39m X[batch_start:batch_start \u001b[38;5;241m+\u001b[39m batch_size], Y[batch_start:batch_start \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[45], line 66\u001b[0m, in \u001b[0;36mNetwork.train_batch\u001b[1;34m(self, X, Y, learning_rate)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     65\u001b[0m     new_weight_gradient, new_bias_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackprop(x, y)\n\u001b[1;32m---> 66\u001b[0m     weight_gradient \u001b[38;5;241m=\u001b[39m [wg \u001b[38;5;241m+\u001b[39m nwg \u001b[38;5;28;01mfor\u001b[39;00m wg, nwg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weight_gradient, new_weight_gradient)]\n\u001b[0;32m     67\u001b[0m     bias_gradient \u001b[38;5;241m=\u001b[39m [bg \u001b[38;5;241m+\u001b[39m nbg \u001b[38;5;28;01mfor\u001b[39;00m bg, nbg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(bias_gradient, new_bias_gradient)]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# C'est ici qu'on calcule les moyennes des gradients calculés\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[45], line 66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     65\u001b[0m     new_weight_gradient, new_bias_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackprop(x, y)\n\u001b[1;32m---> 66\u001b[0m     weight_gradient \u001b[38;5;241m=\u001b[39m [wg \u001b[38;5;241m+\u001b[39m nwg \u001b[38;5;28;01mfor\u001b[39;00m wg, nwg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weight_gradient, new_weight_gradient)]\n\u001b[0;32m     67\u001b[0m     bias_gradient \u001b[38;5;241m=\u001b[39m [bg \u001b[38;5;241m+\u001b[39m nbg \u001b[38;5;28;01mfor\u001b[39;00m bg, nbg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(bias_gradient, new_bias_gradient)]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# C'est ici qu'on calcule les moyennes des gradients calculés\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# La base de données est coupée en deux : entraînement et test.\n",
        "# X, Y = load_mnist_data()      # déjà chargée avant\n",
        "X_train, Y_train = X[:60000], Y[:60000]\n",
        "X_test, Y_test = X[60000:], Y[60000:]\n",
        "\n",
        "net = Network(input_dim=784)\n",
        "net.add_layer(200)\n",
        "net.add_layer(10)\n",
        "\n",
        "accuracy = net.evaluate(X_test, Y_test)\n",
        "print('Performance initiale : {:.2f}%'.format(accuracy * 100.0))\n",
        "\n",
        "for i in range(5):\n",
        "    net.train(X_train, Y_train, steps=1, learning_rate=3.0)\n",
        "    accuracy = net.evaluate(X_test, Y_test)\n",
        "    print('Nouvelle performance : {:.2f}%'.format(accuracy * 100.0))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
